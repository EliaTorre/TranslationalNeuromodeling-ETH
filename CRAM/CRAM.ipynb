{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obmdUYXUn1xG"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGPnEA7emhS2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, train_test_split, StratifiedKFold, KFold\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hE-uJIPfnydO"
   },
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2PTQsLGuu7rn",
    "outputId": "9d3852a1-3fc5-47f2-e891-0ad8bc833505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2O-9_MtBnJ0y"
   },
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/5Drugs_Python.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "id": "PvSysPW-SAFV",
    "outputId": "6d79b4a1-c014-44aa-ada8-2c948774b7a6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-8608032e-4718-45fa-afe7-0d3427d2acd7\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>drug_label</th>\n",
       "      <th>standards_1</th>\n",
       "      <th>standards_2</th>\n",
       "      <th>standards_3</th>\n",
       "      <th>standards_4</th>\n",
       "      <th>standards_5</th>\n",
       "      <th>standards_6</th>\n",
       "      <th>standards_7</th>\n",
       "      <th>standards_8</th>\n",
       "      <th>...</th>\n",
       "      <th>deviants_8748</th>\n",
       "      <th>deviants_8749</th>\n",
       "      <th>deviants_8750</th>\n",
       "      <th>deviants_8751</th>\n",
       "      <th>deviants_8752</th>\n",
       "      <th>deviants_8753</th>\n",
       "      <th>deviants_8754</th>\n",
       "      <th>deviants_8755</th>\n",
       "      <th>deviants_8756</th>\n",
       "      <th>deviants_8757</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>Biperdine</td>\n",
       "      <td>-0.721011</td>\n",
       "      <td>0.905316</td>\n",
       "      <td>-0.268513</td>\n",
       "      <td>0.872179</td>\n",
       "      <td>-0.437147</td>\n",
       "      <td>0.258573</td>\n",
       "      <td>-0.155697</td>\n",
       "      <td>0.862296</td>\n",
       "      <td>...</td>\n",
       "      <td>1.632576</td>\n",
       "      <td>-0.022835</td>\n",
       "      <td>0.915061</td>\n",
       "      <td>-0.499025</td>\n",
       "      <td>0.667729</td>\n",
       "      <td>0.635308</td>\n",
       "      <td>-1.262281</td>\n",
       "      <td>-0.214111</td>\n",
       "      <td>1.228925</td>\n",
       "      <td>1.302340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>1.715061</td>\n",
       "      <td>1.055126</td>\n",
       "      <td>-0.869640</td>\n",
       "      <td>-0.454077</td>\n",
       "      <td>-0.749553</td>\n",
       "      <td>-0.321070</td>\n",
       "      <td>-0.750834</td>\n",
       "      <td>-0.725049</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.097660</td>\n",
       "      <td>-1.254290</td>\n",
       "      <td>0.469221</td>\n",
       "      <td>-1.562479</td>\n",
       "      <td>-0.437813</td>\n",
       "      <td>-1.482711</td>\n",
       "      <td>-2.151189</td>\n",
       "      <td>-1.790298</td>\n",
       "      <td>-1.038511</td>\n",
       "      <td>-0.989457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>Amisulpride</td>\n",
       "      <td>0.865764</td>\n",
       "      <td>1.049517</td>\n",
       "      <td>-0.069502</td>\n",
       "      <td>-0.472154</td>\n",
       "      <td>-0.650986</td>\n",
       "      <td>-0.775461</td>\n",
       "      <td>-0.497490</td>\n",
       "      <td>-1.040329</td>\n",
       "      <td>...</td>\n",
       "      <td>1.084650</td>\n",
       "      <td>-0.648157</td>\n",
       "      <td>0.922619</td>\n",
       "      <td>-1.046926</td>\n",
       "      <td>-0.600638</td>\n",
       "      <td>-1.221202</td>\n",
       "      <td>-0.168258</td>\n",
       "      <td>-1.239763</td>\n",
       "      <td>-1.819047</td>\n",
       "      <td>-1.004539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>Biperdine</td>\n",
       "      <td>0.861297</td>\n",
       "      <td>0.965113</td>\n",
       "      <td>-0.145852</td>\n",
       "      <td>-0.038720</td>\n",
       "      <td>-0.318210</td>\n",
       "      <td>-0.373414</td>\n",
       "      <td>-0.713712</td>\n",
       "      <td>-0.593783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382564</td>\n",
       "      <td>2.847351</td>\n",
       "      <td>-0.711737</td>\n",
       "      <td>1.096458</td>\n",
       "      <td>-0.612047</td>\n",
       "      <td>-0.134576</td>\n",
       "      <td>-2.426027</td>\n",
       "      <td>-1.084767</td>\n",
       "      <td>-0.446180</td>\n",
       "      <td>-0.844795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>Biperdine</td>\n",
       "      <td>-0.871517</td>\n",
       "      <td>0.127640</td>\n",
       "      <td>-0.010109</td>\n",
       "      <td>0.373320</td>\n",
       "      <td>-0.437701</td>\n",
       "      <td>-0.135828</td>\n",
       "      <td>-0.256461</td>\n",
       "      <td>-0.605408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034907</td>\n",
       "      <td>-1.714166</td>\n",
       "      <td>0.884205</td>\n",
       "      <td>-1.344480</td>\n",
       "      <td>-1.194164</td>\n",
       "      <td>-0.761948</td>\n",
       "      <td>-3.541500</td>\n",
       "      <td>-2.257122</td>\n",
       "      <td>-1.308842</td>\n",
       "      <td>-0.375088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 17516 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8608032e-4718-45fa-afe7-0d3427d2acd7')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-8608032e-4718-45fa-afe7-0d3427d2acd7 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-8608032e-4718-45fa-afe7-0d3427d2acd7');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   subject_id   drug_label  standards_1  standards_2  standards_3  \\\n",
       "0         101    Biperdine    -0.721011     0.905316    -0.268513   \n",
       "1         102      Placebo     1.715061     1.055126    -0.869640   \n",
       "2         103  Amisulpride     0.865764     1.049517    -0.069502   \n",
       "3         104    Biperdine     0.861297     0.965113    -0.145852   \n",
       "4         105    Biperdine    -0.871517     0.127640    -0.010109   \n",
       "\n",
       "   standards_4  standards_5  standards_6  standards_7  standards_8  ...  \\\n",
       "0     0.872179    -0.437147     0.258573    -0.155697     0.862296  ...   \n",
       "1    -0.454077    -0.749553    -0.321070    -0.750834    -0.725049  ...   \n",
       "2    -0.472154    -0.650986    -0.775461    -0.497490    -1.040329  ...   \n",
       "3    -0.038720    -0.318210    -0.373414    -0.713712    -0.593783  ...   \n",
       "4     0.373320    -0.437701    -0.135828    -0.256461    -0.605408  ...   \n",
       "\n",
       "   deviants_8748  deviants_8749  deviants_8750  deviants_8751  deviants_8752  \\\n",
       "0       1.632576      -0.022835       0.915061      -0.499025       0.667729   \n",
       "1      -0.097660      -1.254290       0.469221      -1.562479      -0.437813   \n",
       "2       1.084650      -0.648157       0.922619      -1.046926      -0.600638   \n",
       "3       0.382564       2.847351      -0.711737       1.096458      -0.612047   \n",
       "4       0.034907      -1.714166       0.884205      -1.344480      -1.194164   \n",
       "\n",
       "   deviants_8753  deviants_8754  deviants_8755  deviants_8756  deviants_8757  \n",
       "0       0.635308      -1.262281      -0.214111       1.228925       1.302340  \n",
       "1      -1.482711      -2.151189      -1.790298      -1.038511      -0.989457  \n",
       "2      -1.221202      -0.168258      -1.239763      -1.819047      -1.004539  \n",
       "3      -0.134576      -2.426027      -1.084767      -0.446180      -0.844795  \n",
       "4      -0.761948      -3.541500      -2.257122      -1.308842      -0.375088  \n",
       "\n",
       "[5 rows x 17516 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9W882Shansfx"
   },
   "outputs": [],
   "source": [
    "def map_drug_to_number(drug_name):\n",
    "    if drug_name == 'Amisulpride':\n",
    "        return 2\n",
    "    elif drug_name == 'Biperdine':\n",
    "        return 1\n",
    "    elif drug_name == 'Levodopa':\n",
    "        return 3\n",
    "    elif drug_name == 'Galantamine':\n",
    "        return 4\n",
    "    elif drug_name == 'Placebo':\n",
    "        return 0\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vU3MxpNxn5TQ"
   },
   "outputs": [],
   "source": [
    "standards = df.iloc[:, 2:8759].values.reshape((149,63,139))\n",
    "deviants = df.iloc[:, 8759:].values.reshape((149,63,139))\n",
    "labels = df['drug_label'].apply(map_drug_to_number).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dndH7WuaFApE"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iscAanyAqtkQ"
   },
   "outputs": [],
   "source": [
    "class TNU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TNU, self).__init__()\n",
    "\n",
    "         # Channel - Wise Attention Layer\n",
    "        self.r = 32\n",
    "        self.W1 = nn.Parameter(torch.randn(63, self.r))\n",
    "        self.b1 = nn.Parameter(torch.zeros(self.r))\n",
    "        self.W2 = nn.Parameter(torch.randn(self.r, 63))\n",
    "        self.b2 = nn.Parameter(torch.zeros(63))\n",
    "\n",
    "        # Convolution Layer\n",
    "        self.kernel_height = 63\n",
    "        self.kernel_width = 45\n",
    "        self.out_channels = 40\n",
    "        self.in_channels = 1\n",
    "        self.conv = nn.Conv2d(self.in_channels, self.out_channels, kernel_size=(self.kernel_height, self.kernel_width))\n",
    "        self.bn = nn.BatchNorm2d(self.out_channels)\n",
    "\n",
    "        # MaxPooling Layer\n",
    "        self.pool_height = 1\n",
    "        self.pool_width = 75\n",
    "        self.pool_stride = 10\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=(self.pool_height, self.pool_width), stride=(self.pool_stride, self.pool_stride))\n",
    "\n",
    "        # Dropout Layer\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        # LSTM Layer\n",
    "        self.lstm_units = 139\n",
    "        self.lstm = nn.LSTM(input_size=120, hidden_size=self.lstm_units, num_layers=2, dropout=0.5, batch_first=True)\n",
    "\n",
    "        # LSTM - Attention Layer\n",
    "        self.attention_size = 512\n",
    "        self.W3 = nn.Parameter(torch.randn(self.lstm_units, self.attention_size))\n",
    "        self.b3 = nn.Parameter(torch.randn(self.attention_size))\n",
    "        self.W4 = nn.Parameter(torch.randn(self.attention_size, self.lstm_units))\n",
    "        self.b4 = nn.Parameter(torch.randn(self.lstm_units))\n",
    "\n",
    "        # Combined STD/DVT - Attention Layer\n",
    "        self.W5 = nn.Parameter(torch.randn(self.lstm_units*2, self.attention_size))\n",
    "        self.b5 = nn.Parameter(torch.randn(self.attention_size))\n",
    "        self.W6 = nn.Parameter(torch.randn(self.attention_size, self.lstm_units*2))\n",
    "        self.b6 = nn.Parameter(torch.randn(self.lstm_units*2))\n",
    "\n",
    "        # Prediction Layer\n",
    "        self.num_classes = 5\n",
    "        self.softmax_weights = nn.Parameter(torch.randn(2 * self.lstm_units, self.num_classes))\n",
    "        self.softmax_biases = nn.Parameter(torch.randn(self.num_classes))\n",
    "\n",
    "    def forward(self, standards_input, deviants_input, labels=None):\n",
    "        # Mean pooling layer for Standards/Deviants dataset\n",
    "        standards_mean_pool = torch.mean(standards_input, dim=2)  # Shape: [batch_size, 63]\n",
    "        deviants_mean_pool = torch.mean(deviants_input, dim=2)  # Shape: [batch_size, 63]\n",
    "\n",
    "        # Fully connected dimensionality reduction layer for Standards/Deviants dataset\n",
    "        standards_fc1 = torch.tanh(torch.matmul(standards_mean_pool, self.W1) + self.b1)  # Shape: [batch_size, r]\n",
    "        deviants_fc1 = torch.tanh(torch.matmul(deviants_mean_pool, self.W1) + self.b1)  # Shape: [batch_size, r]\n",
    "        standards_fc2 = torch.matmul(standards_fc1, self.W2) + self.b2  # Shape: [batch_size, 63]\n",
    "        deviants_fc2 = torch.matmul(deviants_fc1, self.W2) + self.b2  # Shape: [batch_size, 63]\n",
    "        \n",
    "        # Softmax layer to transform importance of channels to probability distribution\n",
    "        standards_softmax = F.softmax(standards_fc2, dim=1)  # Shape: [batch_size, 63]\n",
    "        deviants_softmax = F.softmax(deviants_fc2, dim=1)  # Shape: [batch_size, 63]\n",
    "        \n",
    "        # Extend model to consider probability weights as recoding information\n",
    "        standards_weighted_samples = standards_input * standards_softmax.unsqueeze(-1)  # Shape: [batch_size, 63, 139]\n",
    "        deviants_weighted_samples = deviants_input * deviants_softmax.unsqueeze(-1)  # Shape: [batch_size, 63, 139]\n",
    "\n",
    "        #Reshaping for CNN layers\n",
    "        batch_size = standards_weighted_samples.shape[0]\n",
    "        standards_weighted_samples_reshaped = torch.reshape(standards_weighted_samples, (batch_size, self.in_channels, 63, 139)) # Shape: [batch_size, 1, 63, 139]\n",
    "        deviants_weighted_samples_reshaped = torch.reshape(deviants_weighted_samples, (batch_size, self.in_channels, 63, 139)) # Shape: [batch_size, 1, 63, 139]\n",
    "        \n",
    "        # Convolutional layer with ELU activation\n",
    "        standards_conv = self.conv(standards_weighted_samples_reshaped) # Shape: [batch_size, 40, 1, 95]\n",
    "        deviants_conv = self.conv(deviants_weighted_samples_reshaped) # Shape: [batch_size, 40, 1, 95]\n",
    "        standards_conv = self.bn(standards_conv) # Shape: [batch_size, 40, 1, 95]\n",
    "        deviants_conv = self.bn(deviants_conv) # Shape: [batch_size, 40, 1, 95]\n",
    "        \n",
    "        standards_conv_elu = F.elu(standards_conv)\n",
    "        deviants_conv_elu = F.elu(deviants_conv)\n",
    "        \n",
    "        # Max-Pooling Layer\n",
    "        standards_pool = self.max_pool(standards_conv_elu) # Shape [batch_size, 40, 1, 3]\n",
    "        deviants_pool = self.max_pool(deviants_conv_elu) # Shape [batch_size, 40, 1, 3]\n",
    "\n",
    "        # Flatten the pooled features\n",
    "        std_pool_shape = standards_pool.size()\n",
    "        dvt_pool_shape = deviants_pool.size()\n",
    "        \n",
    "        standards_flat = standards_pool.view(-1, std_pool_shape[1] * std_pool_shape[2] * std_pool_shape[3]) # Shape [batch_size, 120]\n",
    "        deviants_flat = deviants_pool.view(-1, dvt_pool_shape[1] * dvt_pool_shape[2] * dvt_pool_shape[3]) # Shape [batch_size, 120]\n",
    "\n",
    "        # Dropout Layer\n",
    "        standards_fc_drop = self.dropout(standards_flat) # Shape [batch_size, 120]\n",
    "        deviants_fc_drop = self.dropout(deviants_flat) # Shape [batch_size, 120]\n",
    "        \n",
    "        # Reshape the input for LSTM\n",
    "        standards_reshaped = standards_fc_drop.view(-1, 1, standards_fc_drop.shape[1]) # Shape [batch_size, 1, 120]\n",
    "        deviants_reshaped = deviants_fc_drop.view(-1, 1, standards_fc_drop.shape[1]) # Shape [batch_size, 1, 120]\n",
    "        \n",
    "        # LSTM layer\n",
    "        standards_lstm, _ = self.lstm(standards_reshaped) # Shape [batch_size, 1, 139]\n",
    "        deviants_lstm, _ = self.lstm(deviants_reshaped) # Shape [batch_size, 1, 139]\n",
    "\n",
    "        # Flatten the LSTM output\n",
    "        standards_lstm_flat = standards_lstm.view(-1, self.lstm_units) # Shape [batch_size, 139]\n",
    "        deviants_lstm_flat = deviants_lstm.view(-1, self.lstm_units) # Shape [batch_size, 139]\n",
    "        \n",
    "        # Self-Attention Layer\n",
    "        # Fully-Connected Layer with non-linear activation (Dimensionality Increasing)\n",
    "        standards_fc3 = torch.tanh(torch.matmul(standards_lstm_flat, self.W3)+ self.b3) # Shape [batch_size, 512]\n",
    "        deviants_fc3 = torch.tanh(torch.matmul(deviants_lstm_flat, self.W3)+ self.b3) # Shape [batch_size, 512]\n",
    "\n",
    "        # Fully-Connected Layer (Dimensionality Reducing)\n",
    "        standards_fc4 = torch.matmul(standards_fc3, self.W4) + self.b4 # Shape [batch_size, 139]\n",
    "        deviants_fc4 = torch.matmul(deviants_fc3, self.W4) + self.b4 # Shape [batch_size, 139]\n",
    "\n",
    "        # Softmax layer\n",
    "        standards_softmax_2 = F.softmax(standards_fc4, dim=1)  # Shape [batch_size, 139]\n",
    "        deviants_softmax_2 = F.softmax(deviants_fc4, dim=1)  # Shape [batch_size, 139]\n",
    "        \n",
    "        # Apply attention weights to LSTM output\n",
    "        standards_att_applied = torch.mul(standards_lstm_flat, standards_softmax_2) # Shape [batch_size, 139]\n",
    "        deviants_att_applied = torch.mul(deviants_lstm_flat, deviants_softmax_2) # Shape [batch_size, 139]\n",
    "\n",
    "        # Dropout Layer\n",
    "        standards_fc2_drop = self.dropout(standards_att_applied) # Shape [batch_size, 139]\n",
    "        deviants_fc2_drop = self.dropout(deviants_att_applied) # Shape [batch_size, 139]\n",
    "\n",
    "        # Concatenate the attention-weighted features\n",
    "        combined_features = torch.cat([standards_fc2_drop, deviants_fc2_drop], dim=1) # Shape [batch_size, 278]\n",
    "        \n",
    "        # Calculate attention weights\n",
    "        combined_fc5 = torch.tanh(torch.matmul(combined_features, self.W5) + self.b5) # Shape [batch_size, 278]\n",
    "        combined_fc6 = torch.matmul(combined_fc5, self.W6) + self.b6 # Shape [batch_size, 512]\n",
    "        combined_weights = F.softmax(combined_fc6, dim=1) # Shape [batch_size, 278]\n",
    "\n",
    "        # Apply attention weights to the flattened LSTM output\n",
    "        combined_weighted = combined_features * combined_weights # Shape [batch_size, 278]\n",
    "        #print(\"combined_weighted  shape: \", combined_weighted.shape)\n",
    "\n",
    "        # Calculate logits and predicted probabilities\n",
    "        logits = torch.matmul(combined_weighted, self.softmax_weights) + self.softmax_biases\n",
    "        y_prob = F.softmax(logits, dim=1) # Shape [batch_size, 5]\n",
    "\n",
    "        return y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_xcNSHtsbWzJ",
    "outputId": "47b9f71d-422d-4bc4-a9fa-b15d832a8b70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: 0.4904761904761905\n"
     ]
    }
   ],
   "source": [
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set hyperparameters\n",
    "epochs = 200\n",
    "batch_size = 11\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Datasets\n",
    "standards = torch.Tensor(standards)\n",
    "deviants = torch.Tensor(deviants)\n",
    "labels = torch.Tensor(labels).long()\n",
    "\n",
    "# Define the loss function and optimi\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "# Convert the data to tensors\n",
    "standards = torch.Tensor(standards)\n",
    "deviants = torch.Tensor(deviants)\n",
    "labels = torch.Tensor(labels)\n",
    "\n",
    "# Perform the cross-validation\n",
    "accuracy_history = [] #accuracy per fold\n",
    "cross_val_accuracy = 0 #mean accuracy per fold\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(standards, labels)):\n",
    "\n",
    "    model = TNU().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    standards_train, standards_test = standards[train_index], standards[test_index]\n",
    "    deviants_train, deviants_test = deviants[train_index], deviants[test_index]\n",
    "    labels_train, labels_test = labels[train_index], labels[test_index]\n",
    "\n",
    "    # Create DataLoader for training and test sets\n",
    "    train_dataset = TensorDataset(standards_train, deviants_train, labels_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    test_dataset = TensorDataset(standards_test, deviants_test, labels_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_standards, batch_deviants, batch_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_standards = batch_standards.to(device)\n",
    "            batch_deviants = batch_deviants.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            outputs = model(batch_standards, batch_deviants)\n",
    "\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss/len(train_loader)\n",
    "\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_standards, batch_deviants, batch_labels in test_loader:\n",
    "            batch_standards = batch_standards.to(device)\n",
    "            batch_deviants = batch_deviants.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            outputs = model(batch_standards, batch_deviants)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += batch_labels.size(0)\n",
    "            correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Fold {fold + 1} accuracy: {accuracy}')\n",
    "    accuracy_history.append(accuracy)\n",
    "\n",
    "# Print cross-validation accuracy\n",
    "cross_val_accuracy = np.mean(accuracy_history)\n",
    "print(f\"Cross-Validation Accuracy: {cross_val_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
